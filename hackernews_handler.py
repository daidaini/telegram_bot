#!/usr/bin/env python3
"""
Hacker News API Handler for AI-related articles

This module handles fetching AI-related articles from Hacker News,
fetching their content using MCP fetch server, and analyzing with AI.
"""

import requests
import json
import logging
import re
from datetime import datetime, timedelta
from typing import Dict, List, Optional
from urllib.parse import urlparse
import time
from content_fetcher import ContentFetcher

logger = logging.getLogger(__name__)

class HackerNewsHandler:
    """Handles Hacker News API integration and AI article processing"""

    def __init__(self):
        self.base_url = "https://hacker-news.firebaseio.com/v0"
        self.ai_keywords = [
            'AI', 'artificial intelligence', 'machine learning', 'deep learning',
            'neural network', 'GPT', 'LLM', 'large language model', 'ChatGPT',
            'OpenAI', 'transformer', 'automation', 'robotics', 'computer vision',
            'natural language processing', 'NLP', 'data science', 'algorithm'
        ]
        # Initialize content fetcher
        self.content_fetcher = ContentFetcher()

    def get_new_stories(self, limit: int = 500) -> List[int]:
        """Get latest story IDs from Hacker News"""
        try:
            url = f"{self.base_url}/newstories.json"
            response = requests.get(url, timeout=10)
            response.raise_for_status()
            story_ids = response.json()
            return story_ids[:limit]
        except Exception as e:
            logger.error(f"Error fetching new stories: {e}")
            return []

    def get_story_details(self, story_id: int) -> Optional[Dict]:
        """Get detailed information about a specific story"""
        try:
            url = f"{self.base_url}/item/{story_id}.json"
            response = requests.get(url, timeout=10)
            response.raise_for_status()
            return response.json()
        except Exception as e:
            logger.error(f"Error fetching story {story_id}: {e}")
            return None

    def is_today(self, timestamp: int) -> bool:
        """Check if timestamp is from today"""
        story_date = datetime.fromtimestamp(timestamp)
        today = datetime.now().date()
        return story_date.date() == today

    def is_ai_related(self, text: str) -> bool:
        """Check if text contains AI-related keywords"""
        if not text:
            return False

        text_lower = text.lower()
        return any(keyword.lower() in text_lower for keyword in self.ai_keywords)

    def find_ai_article_today(self) -> Optional[Dict]:
        """Find the latest AI-related article from today"""
        logger.info("Searching for AI-related articles from today...")

        # Get latest stories
        story_ids = self.get_new_stories(200)
        if not story_ids:
            logger.warning("No stories fetched from Hacker News")
            return None

        logger.info(f"Checking {len(story_ids)} stories for AI-related content...")

        # Check each story
        for story_id in story_ids:
            story = self.get_story_details(story_id)
            if not story:
                continue

            # Skip if not from today
            if not self.is_today(story.get('time', 0)):
                continue

            # Skip if not a story (might be a comment or job)
            if story.get('type') != 'story':
                continue

            # Check if AI-related
            title = story.get('title', '')
            text = story.get('text', '')

            if self.is_ai_related(title) or self.is_ai_related(text):
                logger.info(f"Found AI article: {title[:50]}...")
                return story

        logger.info("No AI-related articles found from today")
        return None

    def fetch_article_content(self, url: str) -> Optional[str]:
        """Fetch article content using BeautifulSoup-based content fetcher"""
        try:
            # Validate URL
            parsed = urlparse(url)
            if not parsed.scheme or not parsed.netloc:
                logger.error(f"Invalid URL: {url}")
                return None

            logger.info(f"Fetching content from: {url}")

            # Use the content fetcher with intelligent extraction
            content = self.content_fetcher.fetch_content(
                url=url,
                max_length=15000  # Allow sufficient content for AI analysis
            )

            if content:
                logger.info(f"âœ… Successfully extracted {len(content)} characters")
                return content
            else:
                logger.warning("Content extraction returned empty result")
                return None

        except Exception as e:
            logger.error(f"Error fetching article content: {e}")
            return None

    def analyze_article_with_ai(self, article_data: Dict, content: str, openai_client, model: str) -> Optional[str]:
        """Analyze article content using AI"""
        try:
            title = article_data.get('title', 'No title')
            url = article_data.get('url', '')

            system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIé¢†åŸŸåˆ†æå¸ˆï¼Œæ“…é•¿åˆ†ææŠ€æœ¯æ–‡ç« å¹¶æä¾›æ·±åº¦è§è§£ã€‚

è¯·åˆ†æä»¥ä¸‹AIç›¸å…³çš„æ–‡ç« ï¼Œå¹¶æä¾›ï¼š

1. **æ ¸å¿ƒå†…å®¹æ€»ç»“** - ç”¨2-3å¥è¯æ¦‚æ‹¬æ–‡ç« çš„æ ¸å¿ƒè§‚ç‚¹å’Œä¸»è¦å†…å®¹
2. **æŠ€æœ¯è¦ç‚¹åˆ†æ** - æå–æ–‡ç« ä¸­æ¶‰åŠçš„å…³é”®æŠ€æœ¯ã€æ–¹æ³•æˆ–åˆ›æ–°ç‚¹
3. **è¡Œä¸šå½±å“è¯„ä¼°** - åˆ†æè¿™é¡¹æŠ€æœ¯æˆ–è§‚ç‚¹å¯¹AIè¡Œä¸šå¯èƒ½äº§ç”Ÿçš„å½±å“
4. **ä¸ªäººè§‚ç‚¹** - æä¾›ä½ å¯¹è¿™ä¸ªè¯é¢˜çš„ç‹¬ç«‹è§è§£å’Œè¯„ä»·

è¯·ç”¨ä¸­æ–‡å›å¤ï¼Œä¿æŒä¸“ä¸šã€å®¢è§‚çš„è¯­è°ƒï¼Œé¿å…è¿‡äºæŠ€æœ¯æ€§çš„æœ¯è¯­ï¼Œè®©å¹¿å¤§æŠ€æœ¯çˆ±å¥½è€…éƒ½èƒ½ç†è§£ã€‚

æ–‡ç« å†…å®¹å¦‚ä¸‹ï¼š"""

            user_message = f"""
æ ‡é¢˜ï¼š{title}

é“¾æ¥ï¼š{url}

å†…å®¹ï¼š
{content}
"""

            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_message}
            ]

            response = openai_client.chat_completion(messages, model)
            return response

        except Exception as e:
            logger.error(f"Error analyzing article with AI: {e}")
            return None

    def format_analysis_for_telegram(self, article_data: Dict, analysis: str) -> str:
        """Format the analysis for Telegram message"""
        title = article_data.get('title', 'No title')
        url = article_data.get('url', '')
        score = article_data.get('score', 0)
        comments = article_data.get('descendants', 0)

        message = f"""ğŸ¤– *Hacker News AI æ–‡ç« åˆ†æ*

ğŸ“° **æ ‡é¢˜ï¼š** {title}

ğŸ”— **é“¾æ¥ï¼š** [é˜…è¯»åŸæ–‡]({url})

ğŸ“Š **Hacker Statsï¼š** {score} points | {comments} comments

ğŸ“ **AI åˆ†æç»“æœï¼š**

{analysis}

---
*ğŸ¤– ç”± HN AI æœºå™¨äººè‡ªåŠ¨åˆ†æ*
*ğŸ“… {datetime.now().strftime('%Y-%m-%d %H:%M')}*"""

        return message

    def format_analysis_for_channel(self, article_data: Dict, analysis: str, channel_name: str) -> str:
        """Format the analysis for Telegram channel posting"""
        title = article_data.get('title', 'No title')
        url = article_data.get('url', '')
        score = article_data.get('score', 0)

        message = f"""ğŸ¤– @{channel_name} Hacker News AI ç²¾é€‰

ğŸ”¥ **ä»Šæ—¥çƒ­æ–‡ï¼š** {title}

ğŸ“Š Hacker News: {score} points

ğŸ“ **æ·±åº¦åˆ†æï¼š**

{analysis}

ğŸ”— [é˜…è¯»åŸæ–‡]({url})

---
*ğŸ¤– AI è‡ªåŠ¨åˆ†æ | {datetime.now().strftime('%Y-%m-%d %H:%M')}*"""

        return message